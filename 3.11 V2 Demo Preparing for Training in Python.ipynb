{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Python Libraries\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# O Keras é uma biblioteca de alto nível para construção e treinamento de redes neurais em Python. Ela é integrada ao TensorFlow como seu backend padrão desde o TensorFlow 2.0.\n",
    "import keras\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (Double-Ended Queue) é uma estrutura de dados da biblioteca padrão do Python que permite a manipulação eficiente de dados em ambas as extremidades\n",
    "from collections import deque\n",
    "\n",
    "# uma biblioteca usada para criar barras de progresso em loops, útil para acompanhar o progresso de tarefas demoradas.\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# for dataframe display\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "def display_df(df):\n",
    "    # Puts the scrollbar next to the DataFrame\n",
    "    display(HTML(\"<div style='height: 200px; overflow: auto; width: fit-content'>\" + df.to_html() + \"</div>\"))\n",
    "\n",
    "# for reproducability of answers\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Verifica se há GPUs disponíveis\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Configura apenas a primeira GPU detectada como visível\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        with tf.device('/GPU:0'):\n",
    "            print(\"Usando GPU:0\")\n",
    "    except RuntimeError as e:\n",
    "        # Tratamento de erros caso a GPU já esteja configurada\n",
    "        sys.exit(f\"Erro ao configurar a GPU: {e}\")\n",
    "else:\n",
    "    sys.exit(\"Nenhuma GPU foi detectada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('GOOG_2009-2010_6m_all_features_1d.csv')\n",
    "data = pd.read_csv('google_2008_2009.csv')\n",
    "# display_df(data)\n",
    "\n",
    "dataset = data.reset_index()[['Date','Close','MA5','MA20','BB-upper','BB-lower']]\n",
    "# display_df(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "# Define DQN Model Architecture\n",
    "class DQN(keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Input(shape=(state_size,)))  # Definir explicitamente a entrada\n",
    "        model.add(keras.layers.Dense(units=32, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(units=8, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(action_size, activation=\"linear\"))\n",
    "\n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "        self.model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, window_size, num_features, test_mode=False, model_name=''): # Este é o método inicializador da classe. Ele configura os parâmetros e variáveis do agente.\n",
    "        self.window_size = window_size # How many days of historical data do we want to include in our state representation?\n",
    "        self.num_features = num_features # How many training features do we have?\n",
    "        self.state_size = window_size*num_features # State size includes number of training features per day, and number of lookback days \n",
    "        self.action_size = 3 # 0=hold, 1=buy, 2=sell\n",
    "        self.memory = deque(maxlen=1000) # Bound memory size: once the memory reaches 1000 units, the lefthand values are discarded as righthand values are added\n",
    "        self.inventory = [] # Inventory to hold trades\n",
    "        self.model_name = model_name # filename for saved model checkpoint loading\n",
    "        self.test_mode = test_mode # flag for testing (allows model load from checkpoint model_name)\n",
    "\n",
    "        self.gamma = 0.95 # Fator de desconto. Valores entre 0 e 1. Determina o quanto o agente valoriza recompensas futuras em comparação às imediatas. Exemplo: 0.95 significa que recompensas futuras valem um pouco menos do que recompensas imediatas.\n",
    "        self.epsilon = 1.0 # Taxa de exploração. Inicialmente alta (1.0), o agente escolhe ações aleatórias para explorar o ambiente.\n",
    "        self.epsilon_min = 0.01 # O valor mínimo da taxa de exploração (evita 100% de decisões previsíveis)\n",
    "        self.epsilon_decay = 0.995 # Controla o quanto a taxa de exploração diminui com o tempo\n",
    "        \n",
    "        self.model = keras.models.load_model(model_name) if test_mode else self._model()\n",
    "\n",
    "\n",
    "    # Deep Q Learning (DQL) model\n",
    "    # state_size: Tamanho da entrada da rede (número de informações sobre o estado atual).\n",
    "    # action_size: Número de saídas, correspondente às ações possíveis (manter, comprar, vender).\n",
    "    # DQN: Uma rede neural básica com três camadas (definida em outro lugar no código).\n",
    "    def _model(self): # Constrói o modelo de rede neural para o agente.\n",
    "        model = DQN(self.state_size, self.action_size).model\n",
    "        return model\n",
    "    \n",
    "\n",
    "    # DQL Predict (with input reshaping)\n",
    "    #   Input = State\n",
    "    #   Output = Q-Table of action Q-Values\n",
    "    def get_q_values_for_state(self, state): # Este método prevê os valores Q para um dado estado. Valores Q representam a \"qualidade\" de cada ação.\n",
    "        return self.model.predict(state.flatten().reshape(1, self.state_size))\n",
    "    \n",
    "\n",
    "    # DQL Fit (with input reshaping)\n",
    "    #   Input = State, Target Q-Table \n",
    "    #   Output = MSE Loss between Target Q-Table and Actual Q-Table for State\n",
    "    def fit_model(self, input_state, target_output):\n",
    "        return self.model.fit(input_state.flatten().reshape(1, self.state_size), target_output, epochs=1, verbose=0)    \n",
    "    \n",
    "\n",
    "    # Agent Action Selector\n",
    "    #   Input = State\n",
    "    #   Policy = epsilon-greedy (to minimize possibility of overfitting)\n",
    "    #   Intitially high epsilon = more random, epsilon decay = less random later\n",
    "    #   Output = Action (0, 1, or 2)\n",
    "    def act(self, state): \n",
    "        # Choose any action at random (Probablility = epsilon for training mode, 0% for testing mode)\n",
    "        if not self.test_mode and random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)   \n",
    "        # Choose the action which has the highest Q-value (Probablitly = 1-epsilon for training mode, 100% for testing mode)\n",
    "        options = self.get_q_values_for_state(state)\n",
    "        return np.argmax(options[0]) \n",
    "\n",
    "    # Experience Replay (Learning Function)\n",
    "    #   Input = Batch of (state, action, next_state) tuples\n",
    "    #   Optimal Q Selection Policy = Bellman equation\n",
    "    #   Important Notes = Model fitting step is in this function (fit_model)\n",
    "    #                     Epsilon decay step is in this function\n",
    "    #   Output = Model loss from fitting step\n",
    "    def exp_replay(self, batch_size):\n",
    "        losses = []\n",
    "        mini_batch = []\n",
    "        l = len(self.memory)\n",
    "        for i in range(l - batch_size + 1, l):\n",
    "            mini_batch.append(self.memory[i])\n",
    "            \n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            # reminders: \n",
    "            #   - state is a vector containing close & MA values for the current time step\n",
    "            #   - action is an integer representing the action taken by the act function at the current time step- buy, hold, or sell\n",
    "            #   - reward represents the profit of a given action - it is either 0 (for buy, hold, and sells which loose money) or the profit in dollars (for a profitable sell)\n",
    "            #   - next_state is a vector containing close & MA values for the next time step\n",
    "            #   - done is a boolean flag representing whether or not we are in the last iteration of a training episode (i.e. True when next_state does not exist.)\n",
    "            \n",
    "            if done:\n",
    "                # special condition for last training epoch in batch (no next_state)\n",
    "                optimal_q_for_action = reward  \n",
    "            else:\n",
    "                # target Q-value is updated using the Bellman equation: reward + gamma * max(predicted Q-value of next state)\n",
    "                optimal_q_for_action = reward + self.gamma * np.max(self.get_q_values_for_state(next_state))\n",
    "            # Get the predicted Q-values of the current state\n",
    "            target_q_table = self.get_q_values_for_state(state)  \n",
    "            # Update the output Q table - replace the predicted Q value for action with the target Q value for action \n",
    "            target_q_table[0][action] = optimal_q_for_action\n",
    "            # Fit the model where state is X and target_q_table is Y\n",
    "            history = self.fit_model(state, target_q_table)\n",
    "            losses += history.history['loss']\n",
    "\n",
    "        # define epsilon decay (for the act function)     \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do train_df: 388\n",
      "Tamanho do test_df: 97\n"
     ]
    }
   ],
   "source": [
    "# split dataset df into train (80%) and test (20%) datasets\n",
    "training_rows = int(len(dataset.index)*0.8)\n",
    "train_df = dataset.iloc[:training_rows].set_index(\"Date\")\n",
    "test_df = dataset.iloc[training_rows:].set_index(\"Date\")\n",
    "\n",
    "# display train and test dfs (ensure no overlap)\n",
    "print(f\"Tamanho do train_df: {len(train_df)}\")\n",
    "print(f\"Tamanho do test_df: {len(test_df)}\")\n",
    "\n",
    "# display_df(train_df)\n",
    "# display_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 5)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert train and test dfs to np arrays with dtype=float\n",
    "X_train = train_df.values.astype(float)\n",
    "X_test = test_df.values.astype(float)\n",
    "\n",
    "# print the shape of X_train to remind yourself how many examples and features are in the dataset\n",
    "X_train.shape # X_train.shape retorna as dimensões (shape) do array X_train no formato de uma tupla (n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.disable_interactive_logging()\n",
    "\n",
    "window_size = 1\n",
    "agent = Agent(window_size, num_features=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in agent.__dict__.items():\n",
    "#     if isinstance(value, deque):\n",
    "#         print(f\"{key}: {list(value)[:5]}... (showing first 5 items)\")  # Mostra os primeiros 5 itens do deque\n",
    "#     elif key == \"model\":\n",
    "#         print(f\"{key}: Model object with summary below:\")\n",
    "#         value.summary()\n",
    "#     else:\n",
    "#         print(f\"{key}: {value}\")\n",
    "        \n",
    "# print(\"-> \",agent.model.summary())  # Mostra a arquitetura do modelo\n",
    "\n",
    "# for item in dir(agent):\n",
    "#     print(item)\n",
    "    \n",
    "# print(f\"\\nMemory contents: {list(agent.memory)}\\n\")\n",
    "\n",
    "# for item in vars(agent):\n",
    "#     print(item)\n",
    "    \n",
    "# for layer in agent.model.layers:\n",
    "#     print(f\"Pesos da camada {layer.name}: {layer.get_weights()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format price string\n",
    "def format_price(n):\n",
    "    return ('-$' if n < 0 else '$') + '{0:.2f}'.format(abs(n))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  # Substitui math.exp por np.exp\n",
    "\n",
    "# Plot behavior of trade output\n",
    "def plot_behavior(data_input, bb_upper_data, bb_lower_data, states_buy, states_sell, profit, train=True):\n",
    "    fig = plt.figure(figsize = (15,5))\n",
    "    plt.plot(data_input, color='k', lw=2., label= 'Close Price')\n",
    "    plt.plot(bb_upper_data, color='b', lw=2., label = 'Bollinger Bands')\n",
    "    plt.plot(bb_lower_data, color='b', lw=2.)\n",
    "    plt.plot(data_input, '^', markersize=10, color='r', label = 'Buying signal', markevery = states_buy)\n",
    "    plt.plot(data_input, 'v', markersize=10, color='g', label = 'Selling signal', markevery = states_sell)\n",
    "    plt.title('Total gains: %f'%(profit))\n",
    "    plt.legend()\n",
    "\n",
    "    if train:\n",
    "        plt.xticks(range(0, len(train_df.index.values), int(len(train_df.index.values)/15)), train_df.index.values[0:: int(len(train_df.index.values)/15)], rotation=45, fontsize='small')\n",
    "    else:\n",
    "        plt.xticks(range(0, len(test_df.index.values), int(len(test_df.index.values)/2)), test_df.index.values[0::int(len(test_df.index.values)/2)], rotation=45, fontsize='small')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot training loss\n",
    "def plot_losses(losses, title):\n",
    "    plt.plot(losses)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('MSE Loss Value')\n",
    "    plt.xlabel('batch')\n",
    "    plt.show()\n",
    "\n",
    "def get_state(data, t, n):\n",
    "    \"\"\"\n",
    "    Retorna uma representação de estado de n dias terminando no tempo t.\n",
    "\n",
    "    Parâmetros:\n",
    "    - data: np.ndarray, matriz contendo os dados com shape (dias, features).\n",
    "    - t: int, índice do tempo atual.\n",
    "    - n: int, número de dias (lookback) para a representação do estado.\n",
    "\n",
    "    Retorno:\n",
    "    - np.ndarray: representação do estado com shape (1, n-1, features).\n",
    "    \"\"\"\n",
    "    # Garantir que `data` é um array NumPy\n",
    "    data = np.array(data)\n",
    "\n",
    "    # Determinar o bloco de dados a ser usado\n",
    "    d = t - n\n",
    "    if d >= 0:\n",
    "        block = data[d:t]\n",
    "    else:\n",
    "        block = np.vstack([data[0]] * (abs(d)) + [data[0:t]])  # Repetir o primeiro valor se não houver histórico suficiente\n",
    "\n",
    "    # Calcular as diferenças (deltas) entre dias consecutivos\n",
    "    deltas = block[1:] - block[:-1]\n",
    "\n",
    "    # Aplicar a função sigmoide nos deltas\n",
    "    res = 1 / (1 + np.exp(-deltas))  # Vetorizando a aplicação da função sigmoide\n",
    "\n",
    "    # Expandir a dimensão para manter o formato esperado\n",
    "    return np.expand_dims(res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd99c3544d54f61b485caef5556110d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running episode 0/1:   0%|          | 0/387 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: 0 | Action: 0 | Memory size: 0\n",
      "Id: 1 | Action: 2 | Memory size: 1 | Short Sell: $14.05 | L/P: $0.00\n",
      "Id: 2 | Action: 2 | Memory size: 2 | Short Sell: $12.85 | L/P: $0.00\n",
      "Id: 3 | Action: 0 | Memory size: 3\n",
      "Id: 4 | Action: 0 | Memory size: 4\n",
      "Id: 5 | Action: 0 | Memory size: 5\n",
      "Id: 6 | Action: 2 | Memory size: 6 | Short Sell: $12.58 | L/P: $0.00\n",
      "Id: 7 | Action: 0 | Memory size: 7\n",
      "Id: 8 | Action: 2 | Memory size: 8 | Short Sell: $12.98 | L/P: $0.00\n",
      "Id: 9 | Action: 0 | Memory size: 9\n",
      "Id: 10 | Action: 1  Índice:: 0, Valor: -14.05, short_positions (até agora): [0]\n",
      "Id: 10 | Action: 1  Índice:: 1, Valor: -12.85, short_positions (até agora): [0, 1]\n",
      "Id: 10 | Action: 1  Índice:: 2, Valor: -12.58, short_positions (até agora): [0, 1, 2]\n",
      "Id: 10 | Action: 1  Índice:: 3, Valor: -12.98, short_positions (até agora): [0, 1, 2, 3]\n",
      "Id: 10 | Action: 1 | Memory size: 10 | Repurchase (Cover Short): $13.32 | Profit: $0.74 | L/P: $0.74\n",
      "Id: 11 | Action: 0 | Memory size: 11\n",
      "Id: 12 | Action: 0 | Memory size: 12\n",
      "Id: 13 | Action: 1  Índice:: 0, Valor: -12.85, short_positions (até agora): [0]\n",
      "Id: 13 | Action: 1  Índice:: 1, Valor: -12.58, short_positions (até agora): [0, 1]\n",
      "Id: 13 | Action: 1  Índice:: 2, Valor: -12.98, short_positions (até agora): [0, 1, 2]\n",
      "Id: 13 | Action: 1 | Memory size: 13 | Repurchase (Cover Short): $12.68 | Profit: $0.17 | L/P: $0.91\n",
      "Id: 14 | Action: 0 | Memory size: 14\n",
      "Id: 15 | Action: 1  Índice:: 0, Valor: -12.58, short_positions (até agora): [0]\n",
      "Id: 15 | Action: 1  Índice:: 1, Valor: -12.98, short_positions (até agora): [0, 1]\n",
      "Id: 15 | Action: 1 | Memory size: 15 | Repurchase (Cover Short): $12.52 | Profit: $0.05 | L/P: $0.96\n",
      "Id: 16 | Action: 1  Índice:: 0, Valor: -12.98, short_positions (até agora): [0]\n",
      "Id: 16 | Action: 1 | Memory size: 16 | Repurchase (Cover Short): $12.65 | Profit: $0.33 | L/P: $1.30\n",
      "Id: 17 | Action: 1 | Memory size: 17 | Buy: $12.12 | L/P: $1.30\n",
      "Id: 18 | Action: 0 | Memory size: 18\n",
      "Id: 19 | Action: 2  Índice: 0, Valor: 12.12, buy_positions (até agora): [0]\n",
      "Id: 19 | Action: 2 | Memory size: 19 | Sell: $11.78 | Profit: -$0.34 | L/P: $0.96\n",
      "Id: 20 | Action: 1 | Memory size: 20 | Buy: $11.84 | L/P: $0.96\n",
      "Id: 21 | Action: 1 | Memory size: 21 | Buy: $11.74 | L/P: $0.96\n",
      "Id: 22 | Action: 2  Índice: 0, Valor: 11.84, buy_positions (até agora): [0]\n",
      "Id: 22 | Action: 2  Índice: 1, Valor: 11.74, buy_positions (até agora): [0, 1]\n",
      "Id: 22 | Action: 2 | Memory size: 22 | Sell: $11.38 | Profit: -$0.46 | L/P: $0.50\n",
      "Id: 23 | Action: 1 | Memory size: 23 | Buy: $11.07 | L/P: $0.50\n",
      "Id: 24 | Action: 2  Índice: 0, Valor: 11.74, buy_positions (até agora): [0]\n",
      "Id: 24 | Action: 2  Índice: 1, Valor: 11.07, buy_positions (até agora): [0, 1]\n",
      "Id: 24 | Action: 2 | Memory size: 24 | Sell: $11.15 | Profit: -$0.58 | L/P: -$0.08\n",
      "Id: 25 | Action: 2  Índice: 0, Valor: 11.07, buy_positions (até agora): [0]\n",
      "Id: 25 | Action: 2 | Memory size: 25 | Sell: $10.78 | Profit: -$0.30 | L/P: -$0.38\n",
      "Id: 26 | Action: 1 | Memory size: 26 | Buy: $10.79 | L/P: -$0.38\n",
      "Id: 27 | Action: 0 | Memory size: 27\n",
      "Id: 28 | Action: 1 | Memory size: 28 | Buy: $10.95 | L/P: -$0.38\n",
      "Id: 29 | Action: 2  Índice: 0, Valor: 10.79, buy_positions (até agora): [0]\n",
      "Id: 29 | Action: 2  Índice: 1, Valor: 10.95, buy_positions (até agora): [0, 1]\n",
      "Id: 30 | Action: 0 | Memory size: 30\n",
      "Id: 31 | Action: 0 | Memory size: 31\n",
      "Id: 32 | Action: 2  Índice: 0, Valor: 10.95, buy_positions (até agora): [0]\n",
      "Id: 32 | Action: 2 | Memory size: 32 | Sell: $10.46 | Profit: -$0.50 | L/P: -$0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735085666.527490 2166347 service.cc:146] XLA service 0x7f33ac004b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1735085666.527578 2166347 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 2060, Compute Capability 7.5\n",
      "I0000 00:00:1735085666.911657 2166347 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: 33 | Action: 2 | Memory size: 33 | Short Sell: $10.94 | L/P: -$0.71\n",
      "Id: 34 | Action: 2 | Memory size: 34 | Short Sell: $10.76 | L/P: -$0.71\n",
      "Id: 35 | Action: 2 | Memory size: 35 | Short Sell: $10.80 | L/P: -$0.71\n",
      "Id: 36 | Action: 1  Índice:: 0, Valor: -10.94, short_positions (até agora): [0]\n",
      "Id: 36 | Action: 1  Índice:: 1, Valor: -10.76, short_positions (até agora): [0, 1]\n",
      "Id: 36 | Action: 1  Índice:: 2, Valor: -10.80, short_positions (até agora): [0, 1, 2]\n",
      "Id: 36 | Action: 1 | Memory size: 36 | Repurchase (Cover Short): $11.47 | Profit: -$0.53 | L/P: -$1.24\n",
      "Id: 37 | Action: 2 | Memory size: 37 | Short Sell: $11.23 | L/P: -$1.24\n"
     ]
    }
   ],
   "source": [
    "keras.config.disable_traceback_filtering()  # Desativa logs desnecessários do Keras\n",
    "\n",
    "idx_close = 0  # Índice para preço de fechamento\n",
    "idx_ma5 = 1    # Índice para média móvel de 5 dias\n",
    "idx_ma20 = 2   # Índice para média móvel de 20 dias\n",
    "idx_bb_upper = 3  # Índice para banda superior de Bollinger\n",
    "idx_bb_lower = 4  # Índice para banda inferior de Bollinger\n",
    "\n",
    "# track number of examples in dataset (i.e. number of days to train on)\n",
    "l = X_train[:,0].shape[0] - 1\n",
    "\n",
    "# batch size defines how often to run the exp_replay method\n",
    "batch_size = 32\n",
    "\n",
    "# An episode represents a complete pass over the data.\n",
    "episode_count = 1\n",
    "\n",
    "batch_losses = []\n",
    "num_batches_trained = 0\n",
    "\n",
    "for e in range(episode_count):\n",
    "    state = get_state(X_train, 0, window_size + 1)\n",
    "\n",
    "    # initialize variables\n",
    "    total_profit = 0\n",
    "    total_winners = 0\n",
    "    total_losers = 0\n",
    "    agent.inventory = []\n",
    "    states_sell = []\n",
    "    states_buy = []\n",
    "    \n",
    "    for t in tqdm(range(l), desc=f'Running episode {e}/{episode_count}'):\n",
    "        action = agent.act(state)   \n",
    "        next_state = get_state(X_train, t + 1, window_size + 1)\n",
    "\n",
    "        # initialize reward for the current time step\n",
    "        reward = 0\n",
    "            \n",
    "        if action == 0:\n",
    "            print(f\"Id: {t} | Action: {action} | Memory size: {len(agent.memory)}\")\n",
    "            pass\n",
    "        \n",
    "        elif action == 1:\n",
    "            short_positions = []\n",
    "\n",
    "            for i, pos in enumerate(agent.inventory):\n",
    "                \n",
    "                \n",
    "                if pos < 0:\n",
    "                    short_positions.append(i)  # Adiciona o índice a short_positions\n",
    "                    print(f\"Id: {t} | Action: {action}  Índice:: {i}, Valor: {pos:.2f}, short_positions (até agora): {short_positions}\")\n",
    "            \n",
    "            if short_positions:\n",
    "                # Fecha a posição vendida mais antiga (FIFO = índice menor)\n",
    "                idx_short = short_positions[0]\n",
    "                short_sell_price = abs(agent.inventory.pop(idx_short))\n",
    "                trade_profit = short_sell_price - X_train[t, idx_close]  # Lucro de \"cover short\"\n",
    "                reward = trade_profit\n",
    "                total_profit += trade_profit\n",
    "                if trade_profit >= 0:\n",
    "                    total_winners += trade_profit\n",
    "                else:\n",
    "                    total_losers += trade_profit\n",
    "                print(f\"Id: {t} | Action: {action} \"\n",
    "                      f\"| Memory size: {len(agent.memory)} \"\n",
    "                      f\"| Repurchase (Cover Short): {format_price(X_train[t, idx_close])} \"\n",
    "                      f\"| Profit: {format_price(trade_profit)} | L/P: {format_price(total_profit)}\")\n",
    "                \n",
    "            else:  # Compra normal\n",
    "                buy_price = X_train[t, idx_close]\n",
    "                agent.inventory.append(buy_price)  # Adiciona ao inventário como posição comprada\n",
    "                states_buy.append(t)\n",
    "                print(f\"Id: {t} | Action: {action} | Memory size: {len(agent.memory)} | Buy: {format_price(buy_price)} | L/P: {format_price(total_profit)}\")\n",
    "                \n",
    "        elif action == 2:\n",
    "            buy_positions = []\n",
    "            \n",
    "            for i, pos in enumerate(agent.inventory):\n",
    "                \n",
    "                \n",
    "                if pos > 0:\n",
    "                    buy_positions.append(i)  # Adiciona o índice à lista buy_positions\n",
    "                    print(f\"Id: {t} | Action: {action}  Índice: {i}, Valor: {pos:.2f}, buy_positions (até agora): {buy_positions}\")\n",
    "\n",
    "            if buy_positions:\n",
    "                # Fecha a posição comprada mais antiga\n",
    "                idx_buy = buy_positions[0]\n",
    "                bought_price = agent.inventory.pop(idx_buy)\n",
    "                sell_price = X_train[t, idx_close]\n",
    "                trade_profit = sell_price - bought_price\n",
    "                reward = trade_profit\n",
    "                total_profit += trade_profit\n",
    "                if trade_profit >= 0:\n",
    "                    total_winners += trade_profit\n",
    "                else:\n",
    "                    total_losers += trade_profit\n",
    "                    print(f\"Id: {t} | Action: {action} \"\n",
    "                      f\"| Memory size: {len(agent.memory)} \"\n",
    "                      f\"| Sell: {format_price(sell_price)} \"\n",
    "                      f\"| Profit: {format_price(trade_profit)} | L/P: {format_price(total_profit)}\")\n",
    "                \n",
    "            else:  # Venda a descoberto\n",
    "                short_sell_price = X_train[t, idx_close]\n",
    "                agent.inventory.append(-short_sell_price)  # Posição negativa\n",
    "                states_sell.append(t)\n",
    "                print(f\"Id: {t} | Action: {action} | Memory size: {len(agent.memory)} | Short Sell: {format_price(short_sell_price)} | L/P: {format_price(total_profit)}\")\n",
    "        \n",
    "        # flag for final training iteration\n",
    "        done = True if t == l - 1 else False\n",
    "        # append the details of the state action etc in the memory, to be used by the exp_replay function        \n",
    "        agent.memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        \n",
    "        # print total profit and plot behaviour of the current episode when the episode is finished\n",
    "        if done:\n",
    "        # Vender todos os itens restantes no inventário\n",
    "            while len(agent.inventory) > 0:\n",
    "                position = agent.inventory.pop(0)\n",
    "                if position > 0:  # Fechar compra pendente\n",
    "                    sell_price = X_train[t, idx_close]  # Use o preço de fechamento do último dia\n",
    "                    trade_profit = sell_price - position\n",
    "                else:  # Fechar venda a descoberto pendente\n",
    "                    short_sell_price = abs(position)\n",
    "                    trade_profit = short_sell_price - X_train[t, idx_close]\n",
    "                    \n",
    "                total_profit += trade_profit\n",
    "\n",
    "                if trade_profit >= 0:\n",
    "                    total_winners += trade_profit\n",
    "                else:\n",
    "                    total_losers += trade_profit\n",
    "\n",
    "                states_sell.append(t)\n",
    "                print(f'Forced Close: {format_price(X_train[t, idx_close])} | Profit: {format_price(trade_profit)}')\n",
    "\n",
    "            # Logs e comportamento já existentes\n",
    "            print('--------------------------------')\n",
    "            print(f'Episode {e}')\n",
    "            print(f'Total Profit: {format_price(total_profit)}')\n",
    "            print(f'Total Winners: {format_price(total_winners)}')\n",
    "            print(f'Total Losers: {format_price(total_losers)}')\n",
    "            print('--------------------------------')\n",
    "            plot_behavior(X_train[:, idx_close].flatten(), X_train[:, idx_bb_upper].flatten(), X_train[:, idx_bb_lower].flatten(), states_buy, states_sell, total_profit)\n",
    "            plot_losses(batch_losses[num_batches_trained:len(batch_losses)], f'Episode {e} DQN model loss')\n",
    "            num_batches_trained = len(batch_losses)\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            # when the size of the memory is greater than the batch size, run the exp_replay function on the batch to fit the model and get losses for the batch\n",
    "            losses = agent.exp_replay(batch_size)   \n",
    "            # then sum the losses for the batch and append them to the batch_losses list\n",
    "            batch_losses.append(sum(losses))\n",
    "\n",
    "    agent.model.save(f'model_ep{e}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_test = len(X_test) - 1\n",
    "# done = False\n",
    "# states_sell_test = []\n",
    "# states_buy_test = []\n",
    "\n",
    "# agent = Agent(window_size, num_features=X_test.shape[1], test_mode=True, model_name=f'model_ep{episode_count}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = get_state(X_test, 0, window_size + 1)\n",
    "\n",
    "# # initialize variables\n",
    "# total_profit = 0\n",
    "# total_winners = 0\n",
    "# total_losers = 0\n",
    "# agent.inventory = []\n",
    "\n",
    "# for t in tqdm(range(l_test)):\n",
    "#     action = agent.act(state)   \n",
    "#     next_state = get_state(X_test, t + 1, window_size + 1)\n",
    "\n",
    "#     # initialize reward for the current time step\n",
    "#     reward = 0\n",
    "\n",
    "#     if action == 1: # buy\n",
    "#         # inverse transform to get true buy price in dollars\n",
    "#         buy_price = X_test[t, idx_close]\n",
    "#         agent.inventory.append(buy_price)\n",
    "#         states_buy.append(t)\n",
    "#         print(f'Buy: {format_price(buy_price)}')\n",
    "\n",
    "#     elif action == 2 and len(agent.inventory) > 0: # sell\n",
    "#         bought_price = agent.inventory.pop(0)  \n",
    "#         # inverse transform to get true sell price in dollars\n",
    "#         sell_price = X_test[t, idx_close]\n",
    "\n",
    "#         # define reward as max of profit (close price at time of sell - close price at time of buy) and 0 \n",
    "#         trade_profit = sell_price - bought_price\n",
    "#         reward = max(trade_profit, 0)\n",
    "#         total_profit += trade_profit\n",
    "\n",
    "#         if trade_profit >=0:\n",
    "#             total_winners += trade_profit\n",
    "#         else:\n",
    "#             total_losers += trade_profit\n",
    "            \n",
    "#         states_sell_test.append(t)\n",
    "#         print(f'Sell: {format_price(sell_price)} | Profit: {format_price(trade_profit)}')\n",
    "    \n",
    "#     # flag for final training iteration\n",
    "#     done = True if t == l_test - 1 else False\n",
    "#     # append the details of the state action etc in the memory, to be used by the exp_replay function        \n",
    "#     agent.memory.append((state, action, reward, next_state, done))\n",
    "#     state = next_state\n",
    "#     # print total profit and plot behaviour of the current episode when the episode is finished\n",
    "#     if done:\n",
    "#         print('--------------------------------')\n",
    "#         print(f'Total Profit: {format_price(total_profit)}')\n",
    "#         print(f'Total Winners: {format_price(total_winners)}')\n",
    "#         print(f'Total Losers: {format_price(total_losers)}')\n",
    "#         print('--------------------------------')\n",
    "#         plot_behavior(X_test[:, idx_close].flatten(), X_test[:, idx_bb_upper].flatten(), X_test[:, idx_bb_lower].flatten(), states_buy_test, states_sell_test, total_profit, train=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
